{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Add parent directory to system path\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "sys.path.append(os.path.join(parent_dir, 'core'))\n",
    "\n",
    "# Import from core directory\n",
    "from algorithms import GD, OLSModel\n",
    "from utils import set_randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(lr, x, y, f, admittime, categorical_columns):\n",
    "    \"\"\"\n",
    "    Train OLS model with given learning rate and track results\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    d = x.shape[1]\n",
    "    model = OLSModel(torch.zeros((d,)))\n",
    "    optimizer = GD(model.parameters(), lr=lr)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize tracking arrays\n",
    "    thetas = torch.zeros(n+1, d, dtype=torch.float32)\n",
    "    ys = torch.zeros(n+1, dtype=torch.float32)\n",
    "    fs = torch.zeros(n+1, dtype=torch.float32)\n",
    "    yhats = torch.zeros(n+1, dtype=torch.float32)\n",
    "    losses = torch.zeros(n+1, dtype=torch.float32)\n",
    "    gradients = torch.zeros(n+1, d, dtype=torch.float32)\n",
    "    average_losses = torch.zeros(n+1, dtype=torch.float32)\n",
    "    average_gradients = torch.zeros(n+1, d, dtype=torch.float32)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    for t in range(n):\n",
    "        x_t = x[t]\n",
    "        y_t = y[t]\n",
    "        f_t = f[t]\n",
    "        r_t = y_t - f_t\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        thetas[t+1] = model.theta.detach().cpu()\n",
    "        prediction = model(x_t.to(device))\n",
    "        loss = 0.5 * loss_fn(prediction.squeeze(), r_t.to(device).squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ys[t+1] = y_t.detach().cpu()\n",
    "        fs[t+1] = f_t\n",
    "        yhats[t+1] = f_t + prediction\n",
    "        losses[t+1] = loss.detach().cpu().item()\n",
    "        gradients[t+1] = model.theta.grad.detach().cpu()\n",
    "        average_losses[t+1] = losses[:t+1].mean()\n",
    "        average_gradients[t+1] = gradients[:t+1].mean(dim=0)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'theta': thetas.tolist(),\n",
    "        'y': ys.tolist(),\n",
    "        'f': fs.tolist(),\n",
    "        'yhat': yhats.tolist(),\n",
    "        'loss': losses.tolist(),\n",
    "        'gradient': gradients.tolist(),\n",
    "        'average gradient': average_gradients.tolist(),\n",
    "        'average loss': average_losses.tolist(),\n",
    "        'admittime': [None] + admittime.tolist(),\n",
    "        'lr': lr\n",
    "    })\n",
    "    \n",
    "    # Add categorical columns\n",
    "    for col in categorical_columns:\n",
    "        df[col] = [None] + categorical_columns[col].tolist()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './.cache/gradient_boosting.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m set_randomness(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./.cache/gradient_boosting.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m10000\u001b[39m)  \u001b[38;5;66;03m# Use last 10k samples\u001b[39;00m\n\u001b[1;32m     13\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresiduals\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength_of_stay_float\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    186\u001b[0m     filepath_or_buffer,\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    188\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    189\u001b[0m     is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    190\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    191\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './.cache/gradient_boosting.pkl'"
     ]
    }
   ],
   "source": [
    "# Set flags and parameters\n",
    "experiment_name = \"multigroup\"\n",
    "show_f = False\n",
    "save = True\n",
    "show = False\n",
    "\n",
    "# Set randomness\n",
    "set_randomness(0)\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_pickle('./.cache/gradient_boosting.pkl')\n",
    "data = data.tail(10000)  # Use last 10k samples\n",
    "data['residuals'] = data['length_of_stay_float'] - data['f']\n",
    "\n",
    "# Prepare features\n",
    "categorical_cols = [\"ethnicity\", \"marital_status\"]\n",
    "dummy_df = pd.get_dummies(data[categorical_cols])\n",
    "order = dummy_df.columns.values.tolist()\n",
    "xs = torch.tensor(dummy_df.values.astype(float), dtype=torch.float32)\n",
    "y = torch.tensor(data['length_of_stay_float'].values, dtype=torch.float32)\n",
    "f = torch.tensor(data['f'].values, dtype=torch.float32)\n",
    "\n",
    "# Training parameters\n",
    "lrs = [0, 0.001, 0.01, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models: 100%|██████████| 4/4 [00:31<00:00,  7.93s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train models with different learning rates\n",
    "results = []\n",
    "for lr in tqdm(lrs, desc=\"Training models\"):\n",
    "    result_df = train_model(lr, xs, y, f, data['admittime'], {col: data[col] for col in categorical_cols})\n",
    "    results.append(result_df)\n",
    "\n",
    "# Combine results\n",
    "combined_df = pd.concat(results, ignore_index=True)\n",
    "combined_df['norm of avg grad'] = combined_df['average gradient'].apply(np.linalg.norm, ord=np.inf)\n",
    "\n",
    "# Filter data based on first nonzero gradient\n",
    "combined_df = combined_df[combined_df.admittime > combined_df[combined_df['norm of avg grad'] != 0].admittime.min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Set styling\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sns\u001b[38;5;241m.\u001b[39mset_style(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhitegrid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m sns\u001b[38;5;241m.\u001b[39mset_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m sns\u001b[38;5;241m.\u001b[39mset_palette(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpastel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "# Set styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "sns.set_palette(\"pastel\")\n",
    "\n",
    "# Read from cache\n",
    "combined_df = pd.read_pickle('.cache/mimic_ols_multigroup.pkl')\n",
    "\n",
    "# Create color map\n",
    "lr_cmap_log = plt.colormaps[\"Oranges\"]\n",
    "lr_cmap = LinearSegmentedColormap.from_list(\n",
    "    \"Custom\", \n",
    "    lr_cmap_log(np.logspace(-0.5, 1, 100, base=10))\n",
    ")\n",
    "\n",
    "# Get learning rates and create ordered lists\n",
    "lr_values = sorted(combined_df['lr'].unique())\n",
    "lr_values_reversed = sorted(combined_df['lr'].unique(), reverse=True)\n",
    "\n",
    "# Plot 1: Time Series Analysis\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(23,5), sharex=True, sharey=False)\n",
    "\n",
    "# Define rolling window size\n",
    "window_size = 100\n",
    "\n",
    "# Left plot: Rolling average for predictions\n",
    "# True values (plot these first)\n",
    "true_df = combined_df[combined_df.lr == 0].copy()\n",
    "true_df['y_rolling'] = true_df['y'].rolling(window=window_size, center=True).mean()\n",
    "sns.lineplot(ax=axs[0], data=true_df,\n",
    "            x=\"admittime\", y=\"y_rolling\", color=\"#888888\",\n",
    "            estimator=None, n_boot=0, label=\"True values\")\n",
    "\n",
    "# Predictions for each learning rate (plot these last)\n",
    "pred_df = combined_df.copy()\n",
    "pred_df['yhat_rolling'] = pred_df.groupby('lr')['yhat'].transform(\n",
    "    lambda x: x.rolling(window=window_size, center=True).mean()\n",
    ")\n",
    "_lp = sns.lineplot(ax=axs[0], data=pred_df,\n",
    "                  x=\"admittime\", y=\"yhat_rolling\",\n",
    "                  hue=\"lr\", palette=lr_cmap,\n",
    "                  estimator=None, n_boot=0,\n",
    "                  hue_order=lr_values)\n",
    "\n",
    "if show_f:\n",
    "    axs[0].plot(combined_df.f, color=\"#880000\")\n",
    "    \n",
    "axs[0].set_ylabel(\"Y\")\n",
    "_lp.get_legend().remove()\n",
    "\n",
    "# Middle plot: Gradient norm\n",
    "_lp = sns.lineplot(ax=axs[1], data=combined_df,\n",
    "                  x=\"admittime\", y=\"norm of avg grad\",\n",
    "                  hue=\"lr\", palette=lr_cmap,\n",
    "                  estimator=None, n_boot=0,\n",
    "                  hue_order=lr_values_reversed)\n",
    "axs[1].set_ylabel(\"Norm of bias\")\n",
    "_lp.get_legend().remove()\n",
    "\n",
    "# Right plot: Average loss\n",
    "_lp = sns.lineplot(ax=axs[2], data=combined_df,\n",
    "                  x=\"admittime\", y=\"average loss\",\n",
    "                  hue=\"lr\", palette=lr_cmap,\n",
    "                  estimator=None, n_boot=0,\n",
    "                  hue_order=lr_values_reversed)\n",
    "axs[2].set_ylabel(\"Average loss\")\n",
    "axs[2].legend(loc=\"upper right\", bbox_to_anchor=(1.5,1), title='Learning rate')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"Patient admission date\")\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "sns.despine(top=True, right=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "if save:\n",
    "    os.makedirs(f'./plots/{experiment_name}', exist_ok=True)\n",
    "    plt.savefig(f'./plots/{experiment_name}/series.pdf', bbox_inches=\"tight\")\n",
    "if show:\n",
    "    plt.show()\n",
    "\n",
    "# Plot 2: Stratified Analysis\n",
    "# Drop NaN values in categorical columns\n",
    "for col in categorical_cols:\n",
    "    combined_df = combined_df[~combined_df[col].isna()]\n",
    "\n",
    "# Get unique values for each categorical column\n",
    "uniques = [combined_df[col].unique() for col in categorical_cols]\n",
    "\n",
    "# Set up plot grid based on number of learning rates and categorical columns\n",
    "n_lrs = 2  # We'll show min and max learning rates\n",
    "fig, axs = plt.subplots(nrows=n_lrs, ncols=len(categorical_cols), \n",
    "                       figsize=(20, 8), sharey=True, sharex=True)\n",
    "\n",
    "# Get min and max learning rates\n",
    "lr_min = combined_df.lr.min()\n",
    "lr_max = combined_df.lr.max()\n",
    "lrs_to_plot = [lr_min, lr_max]\n",
    "\n",
    "for i, lr in enumerate(lrs_to_plot):\n",
    "    df_subset = combined_df[combined_df.lr == lr]\n",
    "    \n",
    "    for j, col in enumerate(categorical_cols):\n",
    "        for category in uniques[j]:\n",
    "            cat_data = df_subset[df_subset[col] == category]\n",
    "            gradients = np.array(cat_data.gradient.to_list())\n",
    "            time = np.arange(len(gradients))+1\n",
    "            average_gradient = gradients.cumsum(axis=0)/time[:,None]\n",
    "\n",
    "            sns.lineplot(ax=axs[i,j], x=time, y=np.abs(average_gradient[:,order.index(col + \"_\" + category)]),\n",
    "                        label=category.lower().split('/')[0],\n",
    "                        estimator=None, n_boot=0)\n",
    "        \n",
    "        # Formatting\n",
    "        if i == 0:\n",
    "            axs[i,j].set_title(col.capitalize().replace('_', ' '))\n",
    "        if j == 0:\n",
    "            axs[i,j].set_ylabel(f\"Norm of bias\\nLearning rate {lr}\")\n",
    "        if i == n_lrs-1:\n",
    "            axs[i,j].set_xlabel(\"Patients seen\")\n",
    "            \n",
    "        for tick in axs[i,j].get_xticklabels():\n",
    "            tick.set_rotation(45)\n",
    "        \n",
    "        if i == 1:  # Only keep legend for last row\n",
    "            axs[i,j].get_legend().remove()\n",
    "        else:\n",
    "            sns.move_legend(axs[i,j], \"upper right\")\n",
    "\n",
    "plt.ylim([0, 2])\n",
    "sns.despine(top=True, right=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "if save:\n",
    "    os.makedirs(f'./plots/{experiment_name}', exist_ok=True)\n",
    "    plt.savefig(f'./plots/{experiment_name}/bias.pdf')\n",
    "if show:\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
